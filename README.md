### Improving Tree-LSTM with Tree Attention

In Natural Language Processing (NLP), we often need to extract information from tree topology. Sentence structure can be represented via a dependency tree or a constituency tree structure. For this reason, a variant of LSTMs, named Tree-LSTM, was proposed to work on tree topology. In this paper, we design a generalized attention framework for both dependency and constituency trees by encoding variants of decomposable attention inside a Tree-LSTM cell. We evaluated our models on a semantic relatedness task and achieved notable results compared to Tree-Lstm based methods with no attention as well as other neural and non-neural methods and good results compared to Tree-Lstm based methods with attention.


Published in: 2019 IEEE 13th International Conference on Semantic Computing (ICSC)

https://arxiv.org/abs/1901.00066